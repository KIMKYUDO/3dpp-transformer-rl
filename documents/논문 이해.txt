3차원 포장 문제(3D-PP) 
박스: 직육면체, 90도 직교방향 회전가능
컨테이너 높이는 박스에 따라 조정가능, 가로 세로 고정
** The objective of the problem is to minimize the variable container height**

탐색 전략을 결합한 "휴리스틱 방법" -> "머신러닝 기반 접근"
심층 강화학습(DRL) 모델(정책 네트워크로 트랜스포머 아키텍처를 사용, 해당 네트워크를 PPO(Proximal Policy Optimization)로 학습)
포장 환경의 새로운 상태 표현 **평면 특징(plane features)**을 도입

기존 휴리스틱 방법에 대한 내용
exact algorithms 특징
heuristic methods 특징

spatial representation 종류
Ngoi and Whybrew treats the container as a "three-dimensional matrix" which is divided into multiple two dimensional matrix
Bischoff uses a "two-dimensional matrix" which is the stacked height of boxes at each position of the
top view of the current container
Crainic, Perboli, and Tadei defined "the extreme point concept" based rule for packing boxes inside a three-dimensional container.

머신러닝 기반 접근(DRL)
DRL shows the great potential in combinatorial optimization problems

The network architecture: "encoder–decoder architecture" employing the "Transformer" for both the policy network and the value network 

The training process: PPO, a classic actor–critic DRL method

The two networks use same input(𝑠) , but the outputs of the two networks are different(𝑉𝜋(𝑠𝑡), 𝜋(𝑎𝑡|𝑠𝑡))
𝑠: Observaions of the environment(container, box unpacked)
previous works 𝑠 represent both the observation
=> The advantage: using a single state/ The disadvantage: multiple different states describe the same container situation, increasing the difficulty of the model to learn the policy

Considering the defect, We describe the state of 3D-PP at each step by two different channels: 𝑠_𝑏, 𝑠_𝑐
𝑠_𝑏: 𝑏_𝑖=  (𝑙_𝑖, 𝑤_𝑖, ℎ_𝑖)
𝑠_𝑐: 𝑐_𝑖𝑗 ={ℎ_𝑖𝑗, 𝑒_𝑖𝑗^𝑙, 𝑒_𝑖𝑗^𝑤, 𝑒_𝑖𝑗^−𝑙 , 𝑒_𝑖𝑗^−𝑤 , 𝑓_𝑖𝑗^𝑙, 𝑓 _𝑖𝑗^𝑤 }
{𝑒_𝑖𝑗^𝑙, 𝑒_𝑖𝑗^𝑤, 𝑒_𝑖𝑗^−𝑙 , 𝑒_𝑖𝑗^−𝑤 , 𝑓_𝑖𝑗^𝑙, 𝑓 _𝑖𝑗^𝑤} =  plane features


손실함수가 작아지도록하는 것이 목표
엔트로피함수는 균등한확률이 목표 
Lentropy​= -∑πlogπ라면 균등할수록 커짐
**L = -Lentropy가 되어야함.

L_actor = - min( r_t(theta) * A_hat_t ,
                 clip(r_t(theta), 1 - epsilon, 1 + epsilon) * A_hat_t )
좋은 행동(A>0): clip의 역할 = 손실함수<-min(r*A, (1+𝜖)*A)
r>1 => 좋은 행동의 확률이 커지는 상태, 손실함수<-min(r*A, (1+𝜖)*A)
r<1 => 좋은 행동의 확률이 낮아지는 상태, 확률이 낮아질수록 손실함수가 커짐.
나쁜 행동(A<0): clip의 역할 = -(1-𝜖)*A<손실함수
r>1 => 나쁜 행동의 확률이 커지는 상태, 이는 clip이 막지 않음 과도하게 업데이트해도 됨. 마찬가지로 손실함수 커짐.
r<1 => 나쁜 행동의 확률이 작아지는 상태, -min(r*A, (1-𝜖)*A)<손실함수

Ablation study 머신러닝 모델이나 알고리즘에서 특정요소가 성능에 미치는 영향을 확인하기 위해 해당요소를 제거하거나 변경하면서 그 변화를 관찰하는 연구 방법
corresponding 해당 ~

"downsample the container state", "plane features", "action order(position -> box selection -> box orientation)"
"plane features"=> The model obtain the state of the container in the horizontal direction
"action order" => The number of boxes is higher, the model can receive a larger increase. This point is actually close to the thinking process of human beings.

Position decoder에서 container state에서 (x,y) 추출 후 Embedding을 거치는지?

vs code에서 해보고 cursor에서 최적화하기








