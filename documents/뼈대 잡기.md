action order 

box state container state

downsampling method



actor–critic method: the value network and the policy network

box state 𝑠\_𝑏: 𝑏\_𝑖=  (𝑙\_𝑖, 𝑤\_𝑖, ℎ\_𝑖)

container state 𝑠\_𝑐: 𝑐\_𝑖𝑗 ={ℎ\_𝑖𝑗, 𝑒\_𝑖𝑗^𝑙, 𝑒\_𝑖𝑗^𝑤, 𝑒\_𝑖𝑗^−𝑙 , 𝑒\_𝑖𝑗^−𝑤 , 𝑓\_𝑖𝑗^𝑙, 𝑓 \_𝑖𝑗^𝑤 }



action consists of three distinct types of sb actions: the box selection action, the box orientation action, he position action



reduce the number of action choices using the chain rule.

the function can be decomposed into the product of three probability distributions

π(a\_t | s\_t) 

= π(a\_s\_t, a\_o\_t, a\_p\_t | s\_t) 

= π(a\_p\_t | s\_t) · π(a\_s\_t | a\_p\_t, s\_t) · π(a\_o\_t | a\_p\_t, a\_s\_t, s\_t)



The Transformer structure be used to handle our container state and box state uniformly and to effectively establish a relationship between the box state and the container state.

the encoder–decoder architecture comprises two encoders and three decoders. 

box encoder, container encoder



The three decoders respectively generate three probability distributions over three sub-actions.

position decoder, selection decoder, orientation decoder



1\. Box encoder

s\_b = { b1, b2, …, bn }

𝑏\_𝑖=  (𝑙\_𝑖, 𝑤\_𝑖, ℎ\_𝑖)

generating a 𝑁 × 3 × 𝑑model box feature sequence

the average operation on the second dimension of the sequence, resulting in a new 𝑁×𝑑model sequence.

We feed the sequence into the Transformer encoder without positional encoding to get the box encoding



2\. Container encoder: \*\*downsampling method\*\*

s\_c = {matrix}

𝑐\_𝑖𝑗 ={ℎ\_𝑖𝑗, 𝑒\_𝑖𝑗^𝑙, 𝑒\_𝑖𝑗^𝑤, 𝑒\_𝑖𝑗^−𝑙 , 𝑒\_𝑖𝑗^−𝑤 , 𝑓\_𝑖𝑗^𝑙, 𝑓 \_𝑖𝑗^𝑤 }

\*\*downsampling method\*\*: In our implementation, a 100 × 100 × 7container state is partitioned using a patch size of 10 × 10 and thus theshape of each patch is 10 × 10 × 7. For each patch, we only retain the

feature vector with a maximal value of 𝑒𝑙 × 𝑒𝑤.

We collapse the spacial dimensions of the downsampled container state into one dimension before feeding it

into the encoder. : 10×10×7 -> 100×7 -> 100×𝑑model  

We also add fixed positional encodings to the input to retain positional information of the container

state

100×𝑑model  + positional encodings -> Transformer Encoder : Container Encoding



3\. Position decoder

the container encoding from the container encoder is input into an Transformer decoder as query

and the box encoding is input as key–value. 

And then a fully connected feed-forward network and softmax function to receive the

probability distribution over the position action

sample a position action from theprobability distribution which indicates a coordinate (𝑥, 𝑦) where the next box will be packed.

We extract the feature vector at the point (𝑥, 𝑦) of container encoding and add it to thefeature vector at the point (𝑥, 𝑦) of original container state 𝑠𝑐. Throughfeeding the result into a full connected network we can get the position

embedding.



4\. Selection decoder

we feed the box encoding into the Transformer decoder as query and feed the position embedding as key–value.

we can obtain the probability distribution through a full-connected network and a softmax function.

We sample an action from the probability distribution, we can receive a box from the unpacked box sequence.

Then we generate a box orientation sequence s\_o = { b\_i^s }\_{i=0,1,…,5}, where b\_i^s is the dimensions of the i-th orientation of the selected box

We use a learned embedding to convert the box orientation sequence, resulting in the box orientation embedding.



5\. Orientation decoder

we feed the box orientation embedding into the Transformer decoder as query and feed the position embedding as key–value.

Through a full-connected network and softmax function, the orientation decoder outputs the probability

distribution over the box orientation action

We sample an box orientation action from the probability distribution



6\. Action order

We first decide the position action and finally decide the box orientation action



7\. Value network

The value network uses same architecture like the policy network to output a numerical value

Also, the input is same



8\. Rewards

The objective of 3D-PP is to maximize the utilization rate of a container



reward = g\_{i-1} - g\_i



g\_i = W \* L \* H\_tilde\_i - Σ\_{j=1}^i (w\_j \* l\_j \* h\_j)



where H\_tilde\_i refers to the stacked height of packed boxes at step ; 

Σ\_{j=1}^i (w\_j \* l\_j \* h\_j) refers to the total volume of boxes packed into the container; 

g\_i indicates the gap between box total volume and stacked total volume.



9\. Training



The network is trained using PPO with Generalized Advantage Estimation (GAE)



r\_t(θ) = π\_θ(a\_t | s\_t) / π\_θ\_old(a\_t | s\_t)

L\_CLIP(θ) = Ê\_t \[ min( r\_t(θ) \* Â\_t , clip(r\_t(θ), 1 - ε, 1 + ε) \* Â\_t ) ]

where ε is a hyperparameter, and Â\_t refers to the advantage function calculated by GAE. 

The action a\_t is the combination of three sub-actions, that is, a\_t = (a\_s\_t, a\_o\_t, a\_p\_t).



r\_t(θ) = π\_θ^p(a\_p\_t | s\_t) · π\_θ^s(a\_s\_t | a\_p\_t, s\_t) · π\_θ^o(a\_o\_t | a\_p\_t, a\_s\_t, s\_t)/

π\_θ\_old^p(a\_p\_t | s\_t) · π\_θ\_old^s(a\_s\_t | a\_p\_t, s\_t) · π\_θ\_old^o(a\_o\_t | a\_p\_t, a\_s\_t, s\_t)



the policy network outputs three probability distributions π\_θ^p(a\_p\_t | s\_t), π\_θ^s(a\_s\_t | a\_p\_t, s\_t), and π\_θ^o(a\_o\_t | a\_p\_t, a\_s\_t, s\_t), respectively



The loss function for our DRL method is defined as follows:



L\_actor   = - min( r\_t(θ) \* Â\_t , clip(r\_t(θ), 1 - ε, 1 + ε) \* Â\_t )

L\_critic  = MSE( V\_θc(s\_t), Â\_t + V\_θc(s\_t) )

L\_entropy = - Σ ( π\_θ^s log π\_θ^s + π\_θ^o log π\_θ^o + π\_θ^p log π\_θ^p )

L = L\_actor + L\_critic - β \* L\_entropy

where β is a hyperparameter that is used to balance exploration and exploitation.



10\. Setup and hyper-parameters

**· the length 𝐿 and width 𝑊 of the container are both set to 100**

**· Notice that the problem we study is 3D-PP with available height; therefore the height of thecontainer does not need to be set.**

**· The dimensions (l\_i, w\_i, h\_i) of each box are sampled from:l\_i ∈ \[L/10, L/2], w\_i ∈ \[W/10, W/2], h\_i ∈ \[min(L, W)/10, max(L, W)/2].**

**· Regarding the Transformer architecture, we use two layers for both encoders and decoders.**

**·** We set the dimension of attention embedding 𝑑ℎ = 128.

**·** The number of attention heads of encoders and decoders are set to 4 and 8, respectively.

**·** We use Adam as the optimizer.

**·** The learning rate for the policynetwork is set to 1e−5 and for the value network is set to 1e−4.

**·** In the PPO learning process, the discount factor 𝛾 is set to 0.99 and the decay factor 𝜆 = 0.96 for GAE.

**·** The clip rate 𝜖 for PPO is set to 0.12.

In specific, the UR is defined as follows:

UR = ( Σ\_i (l\_i \* w\_i \* h\_i) ) / (L \* W \* H\_tilde)

We consider three sub-problems with the number of boxes 𝑁 =20, 30, 50, respectively.

We record the average UR on 1024 randomly generated testing instances

For each instance, we sample 16 solutions using the DRL agent and output the best one as the final result.





























