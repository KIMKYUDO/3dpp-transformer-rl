#!/usr/bin/env python3
"""
TF32 vs FP32 ì†ë„ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
TF32ê°€ FP32ë³´ë‹¤ ë¹ ë¥´ë‹¤ëŠ” ê²ƒì„ ëª…í™•í•˜ê²Œ ì¦ëª…í•©ë‹ˆë‹¤.
"""

import torch
import time
import sys
import numpy as np
from typing import List, Tuple

print("=" * 80)
print("TF32 vs FP32 ì†ë„ ë²¤ì¹˜ë§ˆí¬")
print("=" * 80)

# ============================================================================
# 1. í™˜ê²½ í™•ì¸
# ============================================================================

print("\n[1] í™˜ê²½ í™•ì¸")
print("-" * 80)

# PyTorch ë²„ì „
print(f"PyTorch ë²„ì „: {torch.__version__}")
pytorch_version = tuple(map(int, torch.__version__.split('.')[:2]))

# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
if not torch.cuda.is_available():
    print("âŒ ERROR: CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    sys.exit(1)

print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥")

# GPU ì •ë³´
device_name = torch.cuda.get_device_name(0)
compute_capability = torch.cuda.get_device_capability(0)
cc = float(f"{compute_capability[0]}.{compute_capability[1]}")

print(f"GPU: {device_name}")
print(f"Compute Capability: {cc}")

# TF32 ì§€ì› í™•ì¸
if cc >= 8.0:
    print(f"âœ… Ampere ì´ìƒ GPU - TF32 ì§€ì›!")
    tf32_supported = True
elif cc >= 7.0:
    print(f"âš ï¸  Volta/Turing GPU - TF32 ë¯¸ì§€ì› (í…ŒìŠ¤íŠ¸ ì§„í–‰í•˜ì§€ë§Œ ì°¨ì´ ì—†ì„ ê²ƒ)")
    tf32_supported = False
else:
    print(f"âŒ TF32 ë¯¸ì§€ì› GPU - í…ŒìŠ¤íŠ¸ ì˜ë¯¸ ì—†ìŒ")
    tf32_supported = False

# ============================================================================
# 2. ë²¤ì¹˜ë§ˆí¬ í•¨ìˆ˜ë“¤
# ============================================================================

def benchmark_matmul(size: int, dtype: torch.dtype, use_tf32: bool, 
                     iterations: int = 100, warmup: int = 10) -> float:
    """í–‰ë ¬ ê³±ì…ˆ ë²¤ì¹˜ë§ˆí¬"""
    
    device = torch.device('cuda')
    A = torch.randn(size, size, dtype=dtype, device=device)
    B = torch.randn(size, size, dtype=dtype, device=device)
    
    # TF32 ì„¤ì •
    if dtype == torch.float32:
        torch.backends.cuda.matmul.allow_tf32 = use_tf32
        torch.backends.cudnn.allow_tf32 = use_tf32
        if pytorch_version >= (2, 0):
            torch.set_float32_matmul_precision("high" if use_tf32 else "highest")
    
    # Warmup
    for _ in range(warmup):
        C = A @ B
    torch.cuda.synchronize()
    
    # ì¸¡ì •
    torch.cuda.synchronize()
    start = time.perf_counter()
    
    for _ in range(iterations):
        C = A @ B
    
    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start
    
    return elapsed / iterations


def benchmark_linear_layers(batch: int, seq_len: int, hidden: int, 
                            use_tf32: bool, iterations: int = 100) -> float:
    """Linear layer ë²¤ì¹˜ë§ˆí¬ (Transformer ì‹œë®¬ë ˆì´ì…˜)"""
    
    device = torch.device('cuda')
    
    # TF32 ì„¤ì •
    torch.backends.cuda.matmul.allow_tf32 = use_tf32
    torch.backends.cudnn.allow_tf32 = use_tf32
    if pytorch_version >= (2, 0):
        torch.set_float32_matmul_precision("high" if use_tf32 else "highest")
    
    # ê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ (Transformer FFN ìœ ì‚¬)
    model = torch.nn.Sequential(
        torch.nn.Linear(hidden, hidden * 4),
        torch.nn.ReLU(),
        torch.nn.Linear(hidden * 4, hidden)
    ).cuda()
    
    x = torch.randn(batch, seq_len, hidden, device=device)
    
    # Warmup
    for _ in range(10):
        _ = model(x)
    torch.cuda.synchronize()
    
    # ì¸¡ì •
    torch.cuda.synchronize()
    start = time.perf_counter()
    
    for _ in range(iterations):
        _ = model(x)
    
    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start
    
    return elapsed / iterations


def benchmark_attention(batch: int, seq_len: int, hidden: int, 
                       use_tf32: bool, iterations: int = 50) -> float:
    """Self-Attention ë²¤ì¹˜ë§ˆí¬"""
    
    device = torch.device('cuda')
    
    # TF32 ì„¤ì •
    torch.backends.cuda.matmul.allow_tf32 = use_tf32
    torch.backends.cudnn.allow_tf32 = use_tf32
    if pytorch_version >= (2, 0):
        torch.set_float32_matmul_precision("high" if use_tf32 else "highest")
    
    Q = torch.randn(batch, seq_len, hidden, device=device)
    K = torch.randn(batch, seq_len, hidden, device=device)
    V = torch.randn(batch, seq_len, hidden, device=device)
    
    # Warmup
    for _ in range(10):
        scores = Q @ K.transpose(-2, -1) / (hidden ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        out = attn @ V
    torch.cuda.synchronize()
    
    # ì¸¡ì •
    torch.cuda.synchronize()
    start = time.perf_counter()
    
    for _ in range(iterations):
        scores = Q @ K.transpose(-2, -1) / (hidden ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        out = attn @ V
    
    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start
    
    return elapsed / iterations


def benchmark_backward(size: int, use_tf32: bool, iterations: int = 50) -> float:
    """Forward + Backward ë²¤ì¹˜ë§ˆí¬"""
    
    device = torch.device('cuda')
    
    # TF32 ì„¤ì •
    torch.backends.cuda.matmul.allow_tf32 = use_tf32
    torch.backends.cudnn.allow_tf32 = use_tf32
    if pytorch_version >= (2, 0):
        torch.set_float32_matmul_precision("high" if use_tf32 else "highest")
    
    model = torch.nn.Sequential(
        torch.nn.Linear(size, size * 2),
        torch.nn.ReLU(),
        torch.nn.Linear(size * 2, size),
    ).cuda()
    
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    
    x = torch.randn(64, size, device=device)
    target = torch.randn(64, size, device=device)
    
    # Warmup
    for _ in range(10):
        optimizer.zero_grad()
        output = model(x)
        loss = torch.nn.functional.mse_loss(output, target)
        loss.backward()
        optimizer.step()
    torch.cuda.synchronize()
    
    # ì¸¡ì •
    torch.cuda.synchronize()
    start = time.perf_counter()
    
    for _ in range(iterations):
        optimizer.zero_grad()
        output = model(x)
        loss = torch.nn.functional.mse_loss(output, target)
        loss.backward()
        optimizer.step()
    
    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start
    
    return elapsed / iterations


# ============================================================================
# 3. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
# ============================================================================

print("\n[2] ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
print("-" * 80)

results = []

# Test 1: ìˆœìˆ˜ í–‰ë ¬ ê³±ì…ˆ (ë‹¤ì–‘í•œ í¬ê¸°)
print("\nğŸ“Š Test 1: ìˆœìˆ˜ í–‰ë ¬ ê³±ì…ˆ (A @ B)")
print("-" * 80)

matrix_sizes = [512, 1024, 2048, 4096]
for size in matrix_sizes:
    print(f"\ní¬ê¸°: {size}x{size}")
    
    # FP32
    time_fp32 = benchmark_matmul(size, torch.float32, use_tf32=False, iterations=100)
    print(f"  FP32 (highest): {time_fp32*1000:.3f}ms")
    
    # TF32
    time_tf32 = benchmark_matmul(size, torch.float32, use_tf32=True, iterations=100)
    print(f"  TF32 (high):    {time_tf32*1000:.3f}ms")
    
    speedup = time_fp32 / time_tf32
    print(f"  âš¡ ì†ë„ í–¥ìƒ:     {speedup:.2f}x")
    
    results.append({
        'test': f'MatMul {size}x{size}',
        'fp32': time_fp32 * 1000,
        'tf32': time_tf32 * 1000,
        'speedup': speedup
    })


# Test 2: Linear Layers (Transformer FFN)
print("\n\nğŸ“Š Test 2: Linear Layers (Transformer FFN)")
print("-" * 80)

configs = [
    (32, 128, 512),   # (batch, seq_len, hidden)
    (16, 256, 768),
    (8, 512, 1024),
]

for batch, seq_len, hidden in configs:
    print(f"\nì„¤ì •: batch={batch}, seq={seq_len}, hidden={hidden}")
    
    # FP32
    time_fp32 = benchmark_linear_layers(batch, seq_len, hidden, use_tf32=False, iterations=100)
    print(f"  FP32: {time_fp32*1000:.3f}ms")
    
    # TF32
    time_tf32 = benchmark_linear_layers(batch, seq_len, hidden, use_tf32=True, iterations=100)
    print(f"  TF32: {time_tf32*1000:.3f}ms")
    
    speedup = time_fp32 / time_tf32
    print(f"  âš¡ ì†ë„ í–¥ìƒ: {speedup:.2f}x")
    
    results.append({
        'test': f'Linear B{batch}S{seq_len}H{hidden}',
        'fp32': time_fp32 * 1000,
        'tf32': time_tf32 * 1000,
        'speedup': speedup
    })


# Test 3: Self-Attention
print("\n\nğŸ“Š Test 3: Self-Attention (Q @ K^T, Attn @ V)")
print("-" * 80)

attn_configs = [
    (16, 128, 512),
    (8, 256, 768),
    (4, 512, 1024),
]

for batch, seq_len, hidden in attn_configs:
    print(f"\nì„¤ì •: batch={batch}, seq={seq_len}, hidden={hidden}")
    
    # FP32
    time_fp32 = benchmark_attention(batch, seq_len, hidden, use_tf32=False, iterations=50)
    print(f"  FP32: {time_fp32*1000:.3f}ms")
    
    # TF32
    time_tf32 = benchmark_attention(batch, seq_len, hidden, use_tf32=True, iterations=50)
    print(f"  TF32: {time_tf32*1000:.3f}ms")
    
    speedup = time_fp32 / time_tf32
    print(f"  âš¡ ì†ë„ í–¥ìƒ: {speedup:.2f}x")
    
    results.append({
        'test': f'Attention B{batch}S{seq_len}H{hidden}',
        'fp32': time_fp32 * 1000,
        'tf32': time_tf32 * 1000,
        'speedup': speedup
    })


# Test 4: Forward + Backward
print("\n\nğŸ“Š Test 4: Forward + Backward (í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜)")
print("-" * 80)

train_sizes = [256, 512, 1024]
for size in train_sizes:
    print(f"\ní¬ê¸°: {size}")
    
    # FP32
    time_fp32 = benchmark_backward(size, use_tf32=False, iterations=50)
    print(f"  FP32: {time_fp32*1000:.3f}ms")
    
    # TF32
    time_tf32 = benchmark_backward(size, use_tf32=True, iterations=50)
    print(f"  TF32: {time_tf32*1000:.3f}ms")
    
    speedup = time_fp32 / time_tf32
    print(f"  âš¡ ì†ë„ í–¥ìƒ: {speedup:.2f}x")
    
    results.append({
        'test': f'Training {size}',
        'fp32': time_fp32 * 1000,
        'tf32': time_tf32 * 1000,
        'speedup': speedup
    })


# ============================================================================
# 4. ê²°ê³¼ ìš”ì•½
# ============================================================================

print("\n\n" + "=" * 80)
print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
print("=" * 80)

print(f"\n{'í…ŒìŠ¤íŠ¸':<30} {'FP32 (ms)':<12} {'TF32 (ms)':<12} {'ì†ë„ í–¥ìƒ':<12}")
print("-" * 80)

speedups = []
for r in results:
    print(f"{r['test']:<30} {r['fp32']:>10.3f}ms {r['tf32']:>10.3f}ms {r['speedup']:>10.2f}x")
    speedups.append(r['speedup'])

avg_speedup = np.mean(speedups)
min_speedup = np.min(speedups)
max_speedup = np.max(speedups)

print("-" * 80)
print(f"\nğŸ“ˆ í†µê³„:")
print(f"  í‰ê·  ì†ë„ í–¥ìƒ: {avg_speedup:.2f}x")
print(f"  ìµœì†Œ ì†ë„ í–¥ìƒ: {min_speedup:.2f}x")
print(f"  ìµœëŒ€ ì†ë„ í–¥ìƒ: {max_speedup:.2f}x")


# ============================================================================
# 5. ê²°ë¡ 
# ============================================================================

print("\n\n" + "=" * 80)
print("ğŸ¯ ê²°ë¡ ")
print("=" * 80)

if tf32_supported:
    if avg_speedup >= 1.3:
        print(f"\nâœ… TF32ê°€ FP32ë³´ë‹¤ í‰ê·  {avg_speedup:.2f}ë°° ë¹ ë¦…ë‹ˆë‹¤!")
        print(f"   â†’ í–‰ë ¬ ê³±ì…ˆì´ ë§ì€ ë”¥ëŸ¬ë‹ í•™ìŠµì—ì„œ {((avg_speedup-1)*100):.0f}% ì†ë„ í–¥ìƒ!")
        
        if avg_speedup >= 2.0:
            print(f"   â†’ ğŸš€ ë§¤ìš° í° ì†ë„ í–¥ìƒ! ë°˜ë“œì‹œ TF32ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!")
        elif avg_speedup >= 1.5:
            print(f"   â†’ âš¡ ìƒë‹¹í•œ ì†ë„ í–¥ìƒ! TF32 ì‚¬ìš© ê¶Œì¥!")
        else:
            print(f"   â†’ ğŸ’¡ ì¤‘ê°„ ì •ë„ ì†ë„ í–¥ìƒ. TF32 ì‚¬ìš© ì¶”ì²œ!")
    else:
        print(f"\nâš ï¸ TF32ì˜ ì†ë„ í–¥ìƒì´ ê¸°ëŒ€ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤ ({avg_speedup:.2f}x)")
        print("   ê°€ëŠ¥í•œ ì›ì¸:")
        print("   - GPU ì‚¬ìš©ë¥ ì´ ë‚®ìŒ (ë‹¤ë¥¸ ë³‘ëª©)")
        print("   - í–‰ë ¬ í¬ê¸°ê°€ ì‘ìŒ")
        print("   - ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì œí•œ")
else:
    print(f"\nâš ï¸  í˜„ì¬ GPUëŠ” TF32ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print(f"   Compute Capability: {cc} (8.0 ì´ìƒ í•„ìš”)")
    print(f"   TF32 ì§€ì› GPU: RTX 3000/4000 ì‹œë¦¬ì¦ˆ, A100, H100 ë“±")

print("\n" + "=" * 80)

# ============================================================================
# 6. ì¶”ê°€ ì •ë³´
# ============================================================================

print("\n[ì°¸ê³ ] TF32 í™œì„±í™” ë°©ë²•:")
print("-" * 80)
print("""
# train_ppo.py ë§¨ ìœ„ì— ì¶”ê°€:
import torch
torch.set_float32_matmul_precision("high")
torch.backends.cudnn.allow_tf32 = True

# í™•ì¸:
print(torch.get_float32_matmul_precision())  # "high"
print(torch.backends.cuda.matmul.allow_tf32)  # True
""")

print("\në²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")