1. 문제 진단: CriticLoss=inf(NaN)

원인 분석
- 극단적 음수 리워드
- 스케일 불일치: Value 네트워크 출력(작은 값) vs Returns(수백만 단위)
- MSE 폭발

2. Reward Shaping개선 (핵심!)

A. 증분형 보상 설계
reward = gap_reward + ur_reward + height_reward + completion_bonus

B. Gap 정의 개선 (스케일 안정화)
문제: 기존 gap이 H변화에 과민

해결: Heightmap 기반 실제 waste 측정

효과:
- 보상 범위: ±10,000 -> ±500
- gradient 안정화
- PPO 학습 속도↑

C. Height 효율 보상 추가
목적: 같은 부피를 낮은 높이로 달성하도록 유도
핵심: min(raw_above, h_increase)로 슬랩 범위 제한

D. 최종 Reward 구성
역할 분담:
- Gap: 내부 빈틈 제거(미시)
- Height: 슬랩 효율 (중시)
- UR: 전체 목표 (거시) → 중복 없이 보완!

3. Orientation(회전) 버그 수정
중복 제거 안 하는 이유:
- Policy의 OrientationEmbedder가 6개 고정 생성
- 인덱스 불일치 방지

4. 파라미터 값 변경
env.yaml
reward:
	invalid_panalty: -2.0
env:
	max_invalid_attempts: 20
train.yaml
ppo:
	entropy_coef: 0.05
schedule:
	n_steps: 256

