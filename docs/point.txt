from __future__ import annotations
from concurrent.futures import ThreadPoolExecutor
import torch

# 1) matmul(FP32) 내부 정밀도: 'tf32' | 'ieee'
#    'tf32' = 텐서코어 사용(빠름), 'ieee' = 엄격 FP32(정확)
torch.backends.cuda.matmul.fp32_precision = "tf32"

# 2) cuDNN 전체/세부 모듈의 FP32 내부 정밀도
torch.backends.cudnn.fp32_precision = "tf32"        # cuDNN 전반
torch.backends.cudnn.conv.fp32_precision = "tf32"   # 합성곱 경로
torch.backends.cudnn.rnn.fp32_precision  = "tf32"   # RNN 경로

# --- backend 선택 & GUI import 가드 ---
USE_GUI = False  # 훈련 중엔 False 유지

import os
os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True,garbage_collection_threshold:0.8,max_split_size_mb:128"
os.environ.setdefault("MPLBACKEND", "Agg")  # 환경변수로도 고정

if USE_GUI:
    # GUI 쓸 때만 TkAgg + tkinter 가져오기
    os.environ.pop("MPLBACKEND", None)  # 환경변수 강제값 제거
    import matplotlib
    matplotlib.use("TkAgg", force=True)
    import tkinter as tk
    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
else:
    # 기본: 창 없이 파일 저장만 하는 Agg
    os.environ.setdefault("MPLBACKEND", "Agg")
    import matplotlib
    matplotlib.use("Agg", force=True)

# === plotting (자동 저장) ===
try:
    import matplotlib.pyplot as plt
    if not USE_GUI:
        plt.ioff()                              # 인터랙티브 모드 off
    
    import atexit
    atexit.register(lambda: plt.close('all'))
    MATPLOTLIB_OK = True
except Exception:
    MATPLOTLIB_OK = False
    plt = None

import time
import csv
import traceback
from dataclasses import dataclass
from typing import List, Dict, Tuple, cast
from typing import Tuple as _TupleForObs

import yaml
import numpy as np
import torch
from torch import profiler
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
from torch.amp import autocast, GradScaler

# --- add project root to sys.path (for direct script execution) ---
import sys
from pathlib import Path
PROJECT_ROOT = Path(__file__).resolve().parent.parent  # .../projects
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
# -----------------------------------------------------------------


# Local imports
from envs.container_sim import PackingEnv, EnvConfig
from utils.preprocess import compute_plane_features, downsample_patches, flatten_for_encoder
from utils.logger import resolve_resume
from utils.rng_state import get_rng_state, set_rng_state
from utils.plotting import save_packing_3d, save_packing_3d_interactive
from agents.backbone import PolicyBackbone, EncoderConfig
from agents.heads import (
    PositionDecoder, SelectionDecoder, OrientationDecoder,
    PositionEmbeddingBuilder, OrientationEmbedder,
)
from agents.value_head import ValueNet

print("CUDA_ALLOC_CONF =", os.environ.get("PYTORCH_CUDA_ALLOC_CONF"))

# -----------------------------
# 관측 전처리 병렬화 헬퍼
# -----------------------------
def _make_obs_from_env(env) -> _TupleForObs[np.ndarray, np.ndarray, np.ndarray]:
    """
    각 env에서 관측 텐서들을 한 번에 만든다.
    Returns: (raw_flat_np, boxes_np, used_mask_np)
    shapes:  (1,100,7),   (1,N,3),   (1,N)
    """
    if hasattr(env, "container_state7"):
        s7 = env.container_state7()
    else:
        hmap = getattr(env, "heightmap", getattr(env, "height", None))
        if hmap is None:
            raise RuntimeError("Env has no heightmap/height and no container_state7().")
        s7 = compute_plane_features(hmap.astype(np.float32))

    ds7 = downsample_patches(s7, patch=10)
    raw_flat_np = flatten_for_encoder(ds7)[None, ...].astype(np.float32)  # (1,100,7)
    boxes_np    = np.asarray(env.boxes, dtype=np.float32)[None, ...]      # (1,N,3)
    used_np     = env.used.astype(np.bool_)[None, ...]                     # (1,N)
    return raw_flat_np, boxes_np, used_np

# ------------------------------------------------------------
# 디렉터리 안전가드
# ------------------------------------------------------------
os.makedirs(os.path.join("results", "ckpt"), exist_ok=True)
os.makedirs(os.path.join("results", "logs"), exist_ok=True)
os.makedirs(os.path.join("results", "plots"), exist_ok=True)

# ------------------------------------------------------------
# DEBUG 도우미
# ------------------------------------------------------------
DEBUG = True
def dprint(*args):
    if DEBUG:
        print(*args, flush=True)

# ------------------------------------------------------------
# Configs
# ------------------------------------------------------------

@dataclass
class TrainConfig:
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42
    # PPO
    gamma: float = 0.99
    gae_lambda: float = 0.96
    ppo_clip: float = 0.12
    entropy_coef: float = 1e-3
    value_clip_eps: float = 0.2
    critic_loss_type: str = "clipped_mse"
    # LR
    lr_actor: float = 1e-5
    lr_critic: float = 1e-4
    # Rollout / Schedule
    n_steps: int = 128
    num_updates: int = 2          # 데모값 (실험 시 늘리세요)
    epochs_per_update: int = 4
    batch_size: int = 1           # 단일 환경
    # Logging / Checkpoint
    ckpt_dir: str = os.path.join("results", "ckpt")
    log_interval: int = 1
    save_interval: int = 1        # train.yaml의 save_interval 반영 (기본 1)
    # AMP
    grad_accum_steps: int = 1   # 그래디언트 누적 스텝 (메모리 부족 시 >1로 설정)

# ------------------------------------------------------------
# YAML 로더 유틸
# ------------------------------------------------------------
def load_yaml(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

# ------------------------------------------------------------
# Policy wrapper (Actor)
# ------------------------------------------------------------

class PolicyNet(nn.Module):
    def __init__(self, d_model: int = 128):
        super().__init__()
        enc_cfg = EncoderConfig(d_model=d_model, nhead=4, num_layers=2)
        self.backbone = PolicyBackbone(enc_cfg)
        self.pos_dec = PositionDecoder(d_model=d_model, nhead=8, num_layers=2)
        self.pos_emb_builder = PositionEmbeddingBuilder(d_model=d_model)
        self.sel_dec = SelectionDecoder(d_model=d_model, nhead=8, num_layers=2)
        self.orient_embed = OrientationEmbedder(d_model=d_model)
        self.orient_dec = OrientationDecoder(d_model=d_model, nhead=8, num_layers=2)

    @staticmethod
    def _pos_index_to_xy(idx: torch.Tensor, patch: int = 10) -> torch.Tensor:
        # idx: (B,) in [0, 99]; map to (x,y) in container grid by *patch* (10)
        ix = (idx % 10) * patch
        iy = (idx // 10) * patch
        return torch.stack([ix, iy], dim=1)  # (B, 2)

    @staticmethod
    def _num_valid_orients(lwh: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:
        """
        lwh: (B,3) for selected boxes
        Returns: (B,) in {1,3,6}
        rule:
          - all equal -> 1
          - any pair equal -> 3
          - all distinct -> 6
        """
        l, w, h = lwh[..., 0], lwh[..., 1], lwh[..., 2]
        eq_lw = torch.isclose(l, w, atol=eps)
        eq_wh = torch.isclose(w, h, atol=eps)
        eq_lh = torch.isclose(l, h, atol=eps)
        all_eq = eq_lw & eq_wh  # l==w==h
        any_pair = eq_lw | eq_wh | eq_lh
        out = torch.where(all_eq, torch.tensor(1, device=l.device),
              torch.where(any_pair, torch.tensor(3, device=l.device),
                          torch.tensor(6, device=l.device)))
        return out

    def forward_decode(self,
                       boxes: torch.Tensor,          # (B,N,3)
                       cont_flat: torch.Tensor,      # (B,100,7)
                       raw_flat: torch.Tensor,       # (B,100,7) same as cont_flat before proj
                       used_mask: torch.Tensor       # (B,N) bool
                       ) -> Dict[str, torch.Tensor]:
        # Encoders
        box_enc, cont_enc = self.backbone(boxes, cont_flat)  # (B,N,d), (B,100,d)

        # 1) Position
        logits_pos, ctx = self.pos_dec(cont_enc, box_enc)  # (B,100), (B,100,d)

        # 2) Sample position index
        dist_p = Categorical(logits=logits_pos)
        pos_idx = dist_p.sample()                   # (B,)
        pos_logp = F.log_softmax(logits_pos, dim=-1).gather(1, pos_idx.view(-1,1)).squeeze(1)
        pos_entropy = dist_p.entropy()

        # 3) Position embedding
        pos_emb = self.pos_emb_builder(ctx, raw_flat, pos_idx)   # (B,1,d)

        # 4) Selection (mask out used boxes)
        logits_sel, _ = self.sel_dec(box_enc, pos_emb)   # (B,N)
        logits_sel = logits_sel.masked_fill(used_mask, float('-inf'))
        dist_s = Categorical(logits=logits_sel)
        sel_idx = dist_s.sample()                 # (B,)
        sel_logp = F.log_softmax(logits_sel, dim=-1).gather(1, sel_idx.view(-1,1)).squeeze(1)
        sel_entropy = dist_s.entropy()

        # 5) Orientation for the selected box (with VALID-CLASS MASKING)
        B, N, _ = boxes.shape
        gather_idx = sel_idx.view(B,1,1).expand(-1,1,3)  # (B,1,3)
        picked_lwh = boxes.gather(1, gather_idx).squeeze(1)  # (B,3)
        orient_emb = self.orient_embed(picked_lwh)  # (B,6,d)
        logits_or, _ = self.orient_dec(orient_emb, pos_emb)  # (B,6)

        # ---- 유효 오리엔테이션 개수(1/3/6)로 마스킹 ----
        n_valid = self._num_valid_orients(picked_lwh)          # (B,)
        ar6 = torch.arange(6, device=logits_or.device).view(1, 6).expand(B, -1)
        valid_mask = ar6 < n_valid.unsqueeze(1)                 # True=valid
        logits_or = logits_or.masked_fill(~valid_mask, float('-inf'))

        dist_o = Categorical(logits=logits_or)
        orient_idx = dist_o.sample()                            # (B,)
        orient_logp = F.log_softmax(logits_or, dim=-1).gather(1, orient_idx.view(-1,1)).squeeze(1)
        orient_entropy = dist_o.entropy()
        # -----------------------------------------------

        # 6) Compose
        total_logp = pos_logp + sel_logp + orient_logp
        total_entropy = pos_entropy + sel_entropy + orient_entropy

        xy = self._pos_index_to_xy(pos_idx)   # (B,2) int-like

        return {
            'pos_idx': pos_idx, 'sel_idx': sel_idx, 'orient_idx': orient_idx,
            'xy': xy,
            'logp_pos': pos_logp, 'logp_sel': sel_logp, 'logp_or': orient_logp,
            'logp': total_logp, 'entropy': total_entropy,
            # ↓ 그리디 평가용으로 전체 분포(logits)도 넘겨주자
            'logits_pos': logits_pos,     # (B,100)
            'logits_sel': logits_sel,     # (B,N)
            'logits_or':  logits_or,      # (B,6) (마스킹 적용된 분포)
        }

    def evaluate_actions(self,
                         boxes: torch.Tensor,
                         cont_flat: torch.Tensor,
                         raw_flat: torch.Tensor,
                         used_mask: torch.Tensor,
                         pos_idx: torch.Tensor,
                         sel_idx: torch.Tensor,
                         orient_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        box_enc, cont_enc = self.backbone(boxes, cont_flat)
        logits_pos, ctx = self.pos_dec(cont_enc, box_enc)
        pos_logp = F.log_softmax(logits_pos, dim=-1).gather(1, pos_idx.view(-1,1)).squeeze(1)
        dist_p = Categorical(logits=logits_pos)
        pos_ent = dist_p.entropy()

        pos_emb = self.pos_emb_builder(ctx, raw_flat, pos_idx)

        logits_sel, _ = self.sel_dec(box_enc, pos_emb)
        logits_sel = logits_sel.masked_fill(used_mask, float('-inf'))
        sel_logp = F.log_softmax(logits_sel, dim=-1).gather(1, sel_idx.view(-1,1)).squeeze(1)
        dist_s = Categorical(logits=logits_sel)
        sel_ent = dist_s.entropy()

        # 선택된 박스에서 유효 오리엔테이션 개수 산출 → 동일 마스킹
        B, N, _ = boxes.shape
        gather_idx = sel_idx.view(B,1,1).expand(-1,1,3)
        picked_lwh = boxes.gather(1, gather_idx).squeeze(1)     # (B,3)
        orient_emb = self.orient_embed(picked_lwh)
        logits_or, _ = self.orient_dec(orient_emb, pos_emb)

        n_valid = self._num_valid_orients(picked_lwh)          # (B,)
        ar6 = torch.arange(6, device=logits_or.device).view(1, 6).expand(B, -1)
        valid_mask = ar6 < n_valid.unsqueeze(1)
        logits_or = logits_or.masked_fill(~valid_mask, float('-inf'))

        orient_logp = F.log_softmax(logits_or, dim=-1).gather(1, orient_idx.view(-1,1)).squeeze(1)
        dist_o = Categorical(logits=logits_or)
        orient_ent = dist_o.entropy()

        total_logp = pos_logp + sel_logp + orient_logp
        total_entropy = pos_ent + sel_ent + orient_ent
        return total_logp, total_entropy


# ------------------------------------------------------------
# Advantage (GAE) helper
# ------------------------------------------------------------

def compute_gae(rewards: torch.Tensor, values: torch.Tensor, dones: torch.Tensor,
                gamma: float, lam: float, device: str,
                bootstrap_value: torch.Tensor | float | None = None) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    rewards, values, dones: (T,)
    bootstrap_value: s_{T}의 V(s_T) (마지막 next state의 value)  # 미종료 시 사용
    """
    T = rewards.size(0)
    adv = torch.zeros(T, device=device)
    last_val = torch.as_tensor(bootstrap_value, device=device, dtype=values.dtype) if bootstrap_value is not None else torch.tensor(0.0, device=device, dtype=values.dtype)
    last_gae = torch.tensor(0.0, device=device, dtype=values.dtype)
    for t in reversed(range(T)):
        next_val = values[t+1] if t+1 < T else last_val
        next_nonterminal = 1.0 - float(dones[t].item())
        delta = rewards[t] + gamma * next_val * next_nonterminal - values[t]
        last_gae = delta + gamma * lam * next_nonterminal * last_gae
        adv[t] = last_gae
    returns = adv + values
    return returns, adv

# ------------------------------------------------------------
# CSV 초기화(누적-append) & 오프셋 계산
# ------------------------------------------------------------
def init_csv_and_offset(run_name: str) -> Tuple[csv.writer, any, str, int]:
    """
    - results/logs/{run_name}.csv 에 append
    - 없으면 헤더 작성 후 offset=0
    """
    log_dir = os.path.join("results", "logs")
    os.makedirs(log_dir, exist_ok=True)
    log_path = os.path.join(log_dir, f"{run_name}.csv")

    header = ["run","ts","update","T","return_sum","mean_UR","actor_loss","critic_loss","entropy"]
    offset = 0
    file_exists = os.path.exists(log_path) and os.path.getsize(log_path) > 0

    if file_exists:
        last_update = 0
        with open(log_path, "rb") as f:
            f.seek(-2, os.SEEK_END)     # EOF 직전으로 이동
            pos = f.tell() - 1
            while pos > 0:
                f.seek(pos, os.SEEK_SET)
                if f.read(1) == b"\n":
                    break
                pos -= 1
            f.seek(pos + 1, os.SEEK_SET)
            last_line = f.readline().decode().strip()
            
        try:
            last_update = int(last_line.split(",")[2])
            offset = last_update
        except Exception:
            offset = 0
        f = open(log_path, "a", newline="")
        w = csv.writer(f)
    else:
        f = open(log_path, "a", newline="")
        w = csv.writer(f)
        w.writerow(header)

    return w, f, log_path, offset

def make_obs_tensor(env, device):
    # 너의 _make_obs_from_env와 같은 로직을 "단일 env" 기준으로 텐서로 만들어줌
    if hasattr(env, "container_state7"):
        s7 = env.container_state7()
    else:
        hmap = getattr(env, "heightmap", getattr(env, "height", None))
        if hmap is None:
            raise RuntimeError("Env has no heightmap/height and no container_state7().")
        s7 = compute_plane_features(hmap.astype(np.float32))

    ds7 = downsample_patches(s7, patch=10)                 # (10x10,7) 계열
    raw_flat_np = flatten_for_encoder(ds7)[None, ...]      # (1,100,7)

    boxes_np = np.array(env.boxes, dtype=np.float32)[None, ...]  # (1,N,3)
    used_np  = env.used.astype(np.bool_)[None, ...]              # (1,N)

    raw_flat = torch.from_numpy(raw_flat_np).float().to(device)  # (1,100,7)
    cont_flat = raw_flat
    boxes    = torch.from_numpy(boxes_np).float().to(device)     # (1,N,3)
    used_mask= torch.from_numpy(used_np).to(device)               # (1,N) bool
    return raw_flat, cont_flat, boxes, used_mask


def render_eval(policy, env_cfg, device, out_base):
    env = PackingEnv(env_cfg); env.reset()
    policy.eval()
    if max_eval_steps is None:
        # 환경 max_steps가 있으면 그 2~3배 정도로 (정지 보장)
        max_eval_steps = int(getattr(env_cfg, "max_steps", 200)) * 3

    with torch.inference_mode():
        done = False
        steps = 0
        last_placed = -1   # 진행 감지용
        stagnation = 0

        while (not done) and (steps < max_eval_steps):
            raw_flat, cont_flat, boxes, used_mask = make_obs_tensor(env, device)
            out = policy.forward_decode(boxes, cont_flat, raw_flat, used_mask)

            # 1) 모든 박스가 사용됨 → 종료
            if used_mask[0].all():
                break

            # 2) 그리디 선택 (로짓 argmax)
            pos = out["logits_pos"].argmax(-1)  # (1,)
            sel = out["logits_sel"].argmax(-1)  # (1,)
            ori = out["logits_or" ].argmax(-1)  # (1,)

            # 3) (안전) 오리엔트 유효범위 보정
            xy  = PolicyNet._pos_index_to_xy(pos).squeeze(0).tolist()
            x_i, y_i = int(xy[0]), int(xy[1])
            sel_i    = int(sel.item())
            ori_i    = int(ori.item())

            # 4) 스텝 진행
            _, _, done, _ = env.step((x_i, y_i, sel_i, ori_i))

            # 5) 진행 감지 (정체되면 탈출)
            placed_now = len(env.placed_boxes)
            if placed_now == last_placed:
                stagnation += 1
            else:
                stagnation = 0
            last_placed = placed_now
            if stagnation >= 50:   # 50스텝 연속 진행없음 → 탈출
                break

            steps += 1

        boxes = env.placed_boxes
        save_packing_3d(boxes, container=(env_cfg.L, env_cfg.W, env.current_max_height()), out_path=out_base+".png")
        save_packing_3d_interactive(boxes, container=(env_cfg.L, env_cfg.W, env.current_max_height()), out_path=out_base+".html")

# ------------------------------------------------------------
# Training loop
# ------------------------------------------------------------

def train(cfg: TrainConfig, env_cfg: EnvConfig, run_name: str):
    dprint(f"[init] num_updates={cfg.num_updates}, epochs_per_update={cfg.epochs_per_update}, batch_size={cfg.batch_size}")
    os.makedirs(cfg.ckpt_dir, exist_ok=True)
    
    # --- RNG 초기화 (처음 한 번만) ---
    np.random.seed(cfg.seed)
    torch.manual_seed(cfg.seed)
    device = torch.device(cfg.device)

    # CSV 로거 준비 (누적-append)
    csv_w, csv_f, log_path, base_offset = init_csv_and_offset(run_name)
    session_tag = time.strftime("%Y%m%d-%H%M%S")
    dprint(f"[log] {log_path} (base_offset={base_offset})")

    # Env & Nets
    n_steps = cfg.n_steps
    num_envs = getattr(env_cfg, "num_envs", 1)
    envs = [PackingEnv(env_cfg) for _ in range(num_envs)]
    dprint(f"[env] {num_envs} PackingEnv initialized")

    policy = PolicyNet(d_model=128).to(device)
    value = ValueNet(EncoderConfig(d_model=128, nhead=4, num_layers=2), d_model=128, nhead=8, num_layers=2).to(device)

    optim_actor = torch.optim.Adam(policy.parameters(), lr=cfg.lr_actor)
    optim_critic = torch.optim.Adam(value.parameters(), lr=cfg.lr_critic)

    # === AMP scaler 정의 ===
    scaler_actor = GradScaler(enabled=(device.type == "cuda"))
    scaler_critic = GradScaler(enabled=(device.type == "cuda"))

    # === Resume-safe 학습 초기화 ===
    last_milestone, ckpt_path = resolve_resume(cfg, run_name, log_path)
    if last_milestone > 0 and ckpt_path:
        ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)
        policy.load_state_dict(ckpt['policy'])
        value.load_state_dict(ckpt['value'])
        optim_actor.load_state_dict(ckpt['optim_actor'])
        optim_critic.load_state_dict(ckpt['optim_critic'])
        set_rng_state(ckpt['rng_state'])
        start_update = last_milestone + 1
        dprint(f"[resume] from {ckpt_path}, start_update={start_update}")

    else:
        start_update = 1
        dprint("[resume] no checkpoint found — start from scratch")
    
    dprint("[start] training loop")
    try:
        for update in range(start_update, cfg.num_updates + 1):
            dprint(f"[loop] update {update}/{cfg.num_updates} start")

            # Rollout buffers (episode length ≤ N)
            trajs = [
            {k: [] for k in [
                'boxes','cont_flat','raw_flat','used_mask',
                'pos_idx','sel_idx','orient_idx','logp_old',
                'rewards','dones','values'
            ]}
            for _ in range(num_envs)
            ]

            # 초기화
            for env in envs:
                env.reset()
            policy.eval(); value.eval()

            last_done = [False] * num_envs

            # 고정 길이 롤아웃: 정확히 n_steps 스텝 수집
            for t in range(n_steps):
                active_envs = list(range(num_envs))  # done 여부 상관없이 모두 참여

                # ----- 컨테이너 상태 batch 구성 (병렬 전처리) -----
                with ThreadPoolExecutor(max_workers=min(8, len(active_envs))) as pool:
                    futs = [pool.submit(_make_obs_from_env, envs[i]) for i in active_envs]
                    results = [f.result() for f in futs]

                s7_list, boxes_list, used_mask_list = [], [], []
                for raw_flat_np, boxes_np, used_np in results:
                    s7_list.append(raw_flat_np)
                    boxes_list.append(boxes_np)
                    used_mask_list.append(used_np)

                # numpy → torch (batch 합치기)
                raw_flat = torch.from_numpy(np.concatenate(s7_list, axis=0)).float().to(device)   # (B,100,7)
                cont_flat = raw_flat
                boxes = torch.from_numpy(np.concatenate(boxes_list, axis=0)).float().to(device)   # (B,N,3)
                used_mask = torch.from_numpy(np.concatenate(used_mask_list, axis=0)).to(device)  # (B,N)

                # --- Policy batch forward ---
                with torch.inference_mode():
                    out = policy.forward_decode(boxes, cont_flat, raw_flat, used_mask)
                    pos_idx = out['pos_idx']      # (B,)
                    sel_idx = out['sel_idx']      # (B,)
                    orient_idx = out['orient_idx']# (B,)
                    logp = out['logp']            # (B,)
                    xy = out['xy'].cpu().numpy()  # (B,2)
                    
                    V: torch.Tensor = cast(torch.Tensor, value(boxes, cont_flat).squeeze(-1))

                # --- 각 env에 step 적용 ---
                for j, i in enumerate(active_envs):
                    env = envs[i]
                    x, y = map(int, xy[j])
                    obs, reward, done, info = env.step((x, y,
                                                         int(sel_idx[j].item()),
                                                         int(orient_idx[j].item())))
                    reward_scale = 1e-3
                    # Store (현재 스텝의 transition)
                    trajs[i]['boxes'     ].append(boxes[j:j+1].detach().cpu())
                    trajs[i]['cont_flat' ].append(cont_flat[j:j+1].detach().cpu())
                    trajs[i]['raw_flat'  ].append(raw_flat[j:j+1].detach().cpu())
                    trajs[i]['used_mask' ].append(used_mask[j:j+1].detach().cpu())
                    trajs[i]['pos_idx'   ].append(pos_idx[j:j+1].detach().cpu())
                    trajs[i]['sel_idx'   ].append(sel_idx[j:j+1].detach().cpu())
                    trajs[i]['orient_idx'].append(orient_idx[j:j+1].detach().cpu())
                    trajs[i]['logp_old'  ].append(logp[j:j+1].detach().cpu())
                    trajs[i]['rewards'   ].append(torch.tensor([reward * reward_scale], dtype=torch.float32))
                    trajs[i]['dones'     ].append(torch.tensor([done], dtype=torch.float32))
                    trajs[i]['values'    ].append(V[j:j+1].detach().cpu())

                    # 마지막 스텝 done 여부를 기록
                    last_done[i] = bool(done)

                    # done이면 즉시 reset → 다음 스텝 참여
                    if done:
                        env.reset()
            
            with ThreadPoolExecutor(max_workers=min(8, num_envs)) as pool:
                futs = [pool.submit(_make_obs_from_env, envs[i]) for i in range(num_envs)]
                boot = [f.result() for f in futs]

            boot_s7, boot_boxes = [], []
            for raw_flat_np, boxes_np, used_np in boot:
                boot_s7.append(raw_flat_np)
                boot_boxes.append(boxes_np)

            boot_raw  = torch.from_numpy(np.concatenate(boot_s7, axis=0)).float().to(device)    # (B,100,7)
            boot_cont = boot_raw
            boot_box  = torch.from_numpy(np.concatenate(boot_boxes, axis=0)).float().to(device) # (B,N,3)

            with torch.inference_mode():
                boot_V = value(boot_box, boot_cont).squeeze(-1).detach().cpu()  # (num_envs,)

            # --- 시각화 저장 ---
            if update % cfg.log_interval == 0:
                boxes = envs[0].placed_boxes
                out_base = f"results/plots/packing_u{update:05d}"
                save_packing_3d(boxes, container=(env_cfg.L, env_cfg.W, envs[0].current_max_height()),
                                out_path=out_base+".png")
                save_packing_3d_interactive(boxes, container=(env_cfg.L, env_cfg.W, envs[0].current_max_height()),
                                            out_path=out_base+".html")

            # --- 에피소드 통계 ---
            return_sum = float(np.mean([sum(r.item() for r in traj['rewards']) for traj in trajs]))
            ur = float(np.mean([env.utilization_rate() for env in envs]))
            dprint(f"[collect] trajectories ready | T={n_steps * num_envs} | return={return_sum:.2f} | UR={ur:.4f}")

            # --- rollout 병합 ---
            traj = {k: torch.cat([torch.cat(trajs[i][k], dim=0) for i in range(num_envs)], dim=0)
                    for k in trajs[0].keys()}
            
            # === 여기서 '항상' 텐서로 준비 ===
            device_opt = dict(device=device, non_blocking=True)
            rewards_T      = traj['rewards'   ].squeeze(-1).to(**device_opt)         # (T,)
            dones_T        = traj['dones'     ].squeeze(-1).to(**device_opt)         # (T,)
            values_T       = traj['values'    ].squeeze(-1).to(**device_opt)         # (T,)
            logp_old_T     = traj['logp_old'  ].squeeze(-1).to(**device_opt)         # (T,)
            pos_idx_T      = traj['pos_idx'   ].squeeze(-1).to(**device_opt).long()  # (T,)
            sel_idx_T      = traj['sel_idx'   ].squeeze(-1).to(**device_opt).long()  # (T,)
            orient_idx_T   = traj['orient_idx'].squeeze(-1).to(**device_opt).long()  # (T,)

            T = n_steps * num_envs

            # env별로 구간을 쪼개어 GAE 계산 (부트스트랩 포함)
            offset = 0
            rets_list, adv_list = [], []
            for i in range(num_envs):
                Ti = len(trajs[i]['rewards'])  # 해당 env의 길이
                r_i = rewards_T[offset:offset+Ti]
                d_i = dones_T[offset:offset+Ti]
                v_i = values_T[offset:offset+Ti]

                # 마지막이 terminal이면 0, truncated(미종료)이면 V(next)
                bootstrap = 0.0 if (Ti == 0 or last_done[i]) else float(boot_V[i].item())

                ret_i, adv_i = compute_gae(r_i, v_i, d_i, cfg.gamma, cfg.gae_lambda, cfg.device, bootstrap_value=bootstrap)
                rets_list.append(ret_i)
                adv_list.append(adv_i)
                offset += Ti

            returns_T = torch.cat(rets_list, dim=0)
            adv_T     = torch.cat(adv_list, dim=0)
            adv_T = (adv_T - adv_T.mean()) / (adv_T.std() + 1e-8)

            # ---- CPU 텐서로 유지 + pinned 준비 ----
            boxes_B = traj['boxes'].squeeze(1).contiguous().pin_memory()      # (T, N, 3)
            cont_B  = traj['cont_flat'].squeeze(1).contiguous().pin_memory()  # (T, 100, 7)
            raw_B   = traj['raw_flat'].squeeze(1).contiguous().pin_memory()   # (T, 100, 7)
            used_B  = traj['used_mask'].squeeze(1).contiguous().pin_memory()  # (T, N) or (T,)

            pos_B = pos_idx_T
            sel_B = sel_idx_T
            or_B  = orient_idx_T

            adv_T     = adv_T.to(device)
            returns_T = returns_T.to(device)
            logp_old_T= logp_old_T.to(device)

            accum = getattr(cfg, "grad_accum_steps", 1)

            policy.train(); value.train()

            for epoch in range(cfg.epochs_per_update):
                idx = torch.randperm(T)

                # 누적을 위해 zero_grad를 에폭 시작에서 1회
                optim_actor.zero_grad(set_to_none=True)
                optim_critic.zero_grad(set_to_none=True)

                mb = 0
                for start in range(0, T, cfg.batch_size):
                    b_cpu = idx[start:start+cfg.batch_size]            # CPU index
                    b_gpu = b_cpu.to(device, non_blocking=True)        # GPU index (GPU 소스용)

                    # --- CPU 소스 (pinned) → CPU 인덱싱 → GPU로 비동기 복사 ---
                    boxes_b = boxes_B.index_select(0, b_cpu).to(device, non_blocking=True)
                    cont_b  = cont_B .index_select(0, b_cpu).to(device, non_blocking=True)
                    raw_b   = raw_B  .index_select(0, b_cpu).to(device, non_blocking=True)
                    used_b  = used_B .index_select(0, b_cpu).to(device, non_blocking=True).bool()

                    # --- GPU 소스 → GPU 인덱싱 ---
                    pos_b       = pos_B      .index_select(0, b_gpu)
                    sel_b       = sel_B      .index_select(0, b_gpu)
                    or_b        = or_B       .index_select(0, b_gpu)
                    logp_old_b  = logp_old_T .index_select(0, b_gpu)
                    adv_b       = adv_T      .index_select(0, b_gpu)
                    rets_b      = returns_T  .index_select(0, b_gpu)

                    with autocast(device_type=device.type, enabled=(device.type=="cuda")):
                        logp_new, ent = policy.evaluate_actions(
                            boxes_b, cont_b, raw_b, used_b, pos_b, sel_b, or_b
                        )
                        V_pred = value(boxes_b, cont_b).squeeze(-1)

                        ratio = (logp_new - logp_old_b.to(logp_new.dtype)).exp()
                        surr1 = ratio * adv_b
                        surr2 = torch.clamp(ratio, 1 - cfg.ppo_clip, 1 + cfg.ppo_clip) * adv_b
                        actor_loss   = -torch.min(surr1, surr2).mean()

                        # --- PPO-style value clipping ---
                        V_old_b = values_T.index_select(0, b_gpu).detach().to(V_pred.dtype)  # 기존 값 고정
                        value_clip_eps = getattr(cfg, "value_clip_eps", 0.2)

                        V_pred_clipped = V_old_b + (V_pred - V_old_b).clamp(-value_clip_eps, value_clip_eps)
                        
                        rets_b = rets_b.to(V_pred.dtype)  # 손실 계산 dtype 통일
                        critic_loss_1 = F.mse_loss(V_pred, rets_b)
                        critic_loss_2 = F.mse_loss(V_pred_clipped, rets_b)
                        critic_loss = torch.max(critic_loss_1, critic_loss_2)
                        
                        entropy_mean = ent.mean()
                        total_actor_loss = actor_loss - cfg.entropy_coef * entropy_mean
                    
                    # --- NaN guard: backward 전에! ---
                    if (not torch.isfinite(actor_loss)) or (not torch.isfinite(critic_loss)) or (not torch.isfinite(entropy_mean)):
                        dprint("[nan] detected — skipping step",
                            "| total_actor:", total_actor_loss.item() if torch.isfinite(total_actor_loss) else "NaN",
                            "| critic:", critic_loss.item() if torch.isfinite(critic_loss) else "NaN")
                        optim_actor.zero_grad(set_to_none=True)
                        optim_critic.zero_grad(set_to_none=True)
                        # 누적 카운트 증가/step 판단 전에 건너뛰기
                        continue

                    # ---- 그래디언트 누적 (loss/accum) ----
                    scaler_actor.scale(total_actor_loss / accum).backward()
                    scaler_critic.scale(critic_loss / accum).backward()

                    # ---- step 타이밍: 매 accum 스텝마다 ----
                    mb += 1
                    should_step = ((mb % accum) == 0) or (start + cfg.batch_size >= T)

                    if should_step:
                        # Actor step
                        scaler_actor.unscale_(optim_actor)
                        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
                        scaler_actor.step(optim_actor); scaler_actor.update()
                        optim_actor.zero_grad(set_to_none=True)

                        # Critic step
                        scaler_critic.unscale_(optim_critic)
                        torch.nn.utils.clip_grad_norm_(value.parameters(), 1.0)
                        scaler_critic.step(optim_critic); scaler_critic.update()
                        optim_critic.zero_grad(set_to_none=True)

            # --- 로그 ---
            if update % cfg.log_interval == 0:
                dprint(f"[update {update}] T={T} | Return={returns_T.sum().item():.1f} | "
                    f"ActorLoss={actor_loss.item():.4f} | CriticLoss={critic_loss.item():.4f} | "
                    f"Entropy={entropy_mean.item():.4f}")
                
                # 그리디 평가 (완주) 스냅샷
                try:
                    render_eval(policy, env_cfg, device, f"results/plots/eval_u{update:05d}")
                except Exception as e:
                    dprint("[plot] eval render failed:", e)
            
            # CSV 로그 기록 (누적)
            try:
                csv_w.writerow([
                    run_name, session_tag, update, T,
                    f"{return_sum:.4f}", f"{ur:.6f}",
                    f"{actor_loss.item():.6f}", f"{critic_loss.item():.6f}", f"{entropy_mean.item():.6f}"
                ])
                csv_f.flush()
            except Exception as e:
                dprint("[warn] CSV write failed:", e)

            # --- 체크포인트 저장 ---
            if update % cfg.save_interval == 0:
                ckpt_path = os.path.join(cfg.ckpt_dir, f'{run_name}_u{update:05d}.pt')
                torch.save({
                    'policy': policy.state_dict(),
                    'value': value.state_dict(),
                    'optim_actor': optim_actor.state_dict(),
                    'optim_critic': optim_critic.state_dict(),
                    "rng_state": get_rng_state(),
                    'cfg': cfg.__dict__,
                    'env_cfg': env_cfg.__dict__,
                }, ckpt_path)
                dprint(f"[save] ckpt saved to: {ckpt_path}")

        dprint("[done] training loop complete")

    except KeyboardInterrupt:
        dprint("[interrupt] Training interrupted by user (Ctrl+C)")
        dprint(f"[save] interrupted ckpt saved: {ckpt_path}")

    except Exception:
        dprint("[fatal] exception in training loop")
        traceback.print_exc()
        raise
    finally:
        try:
            csv_f.close()
        except Exception:
            pass

        # === 학습 종료 시 자동 플롯 저장 ===
        if MATPLOTLIB_OK:
            try:
                updates, rets, urs = [], [], []
                with open(log_path, "r", newline="") as f:
                    rd = csv.DictReader(f)
                    for row in rd:
                        try:
                            x = int(row.get("update"))
                            updates.append(x)
                            rets.append(float(row["return_sum"]))
                            urs.append(float(row["mean_UR"]))
                        except Exception:
                            pass

                out_dir = os.path.join("results", "plots")
                os.makedirs(out_dir, exist_ok=True)

                plt.figure()
                plt.plot(updates, rets)
                plt.title(f"Return (sum per update) — {run_name}")
                plt.xlabel("update"); plt.ylabel("return_sum")
                plt.tight_layout()
                ret_png = os.path.join(out_dir, f"{run_name}_return.png")
                plt.savefig(ret_png, dpi=150)
                plt.close()

                plt.figure()
                plt.plot(updates, urs)
                plt.title(f"Utilization Rate — {run_name}")
                plt.xlabel("update"); plt.ylabel("UR")
                plt.tight_layout()
                ur_png = os.path.join(out_dir, f"{run_name}_ur.png")
                plt.savefig(ur_png, dpi=150)
                plt.close()

                dprint(f"[plot] saved: {ret_png}")
                dprint(f"[plot] saved: {ur_png}")
            except Exception as e:
                dprint("[warn] plotting failed:", e)
        else:
            dprint("[plot] matplotlib not available; skip saving plots")

# ------------------------------------------------------------
# main
# ------------------------------------------------------------
if __name__ == "__main__":
    # 기본값
    tcfg = TrainConfig()
    ecfg = EnvConfig(L=100, W=100, N=20, seed=None, num_envs=4, max_steps=200)

    # ---------- YAML 설정 연결 ----------
    # configs/env.yaml 로드
    try:
        env_y = load_yaml(os.path.join("configs", "env.yaml"))
        L = int(env_y["container"]["length"])
        W = int(env_y["container"]["width"])
        seed = int(env_y["container"]["seed"])
        N = int(env_y["boxes"]["count"])
        num_envs = int(env_y["env"]["num_envs"])
        max_steps = int(env_y["env"]["max_steps"])
        ecfg = EnvConfig(L=L, W=W, N=N, seed=seed, num_envs=num_envs, max_steps=max_steps)
        dprint(f"[cfg] env.yaml loaded → L={L}, W={W}, N={N}, seed={seed}, num_envs={num_envs}, max_steps={max_steps}")
    except Exception as e:
        dprint("[cfg] env.yaml not loaded, using defaults:", e)

    # configs/train.yaml 로드
    try:
        train_y = load_yaml(os.path.join("configs", "train.yaml"))
        ppo = train_y.get("ppo", {})
        opt = train_y.get("optimizer", {})
        sch = train_y.get("schedule", {})
        amp = train_y.get("amp", {})

        tcfg.gamma        = float(ppo.get("gamma", tcfg.gamma))
        tcfg.gae_lambda   = float(ppo.get("gae_lambda", tcfg.gae_lambda))
        tcfg.ppo_clip     = float(ppo.get("clip_epsilon", tcfg.ppo_clip))
        tcfg.entropy_coef = float(ppo.get("entropy_coef", tcfg.entropy_coef))

        tcfg.lr_actor     = float(opt.get("policy_lr", tcfg.lr_actor))
        tcfg.lr_critic    = float(opt.get("value_lr", tcfg.lr_critic))

        tcfg.n_steps           = int(sch.get("n_steps", tcfg.n_steps))
        tcfg.num_updates       = int(sch.get("num_updates", tcfg.num_updates))
        tcfg.epochs_per_update = int(sch.get("epochs_per_update", tcfg.epochs_per_update))
        tcfg.batch_size        = int(sch.get("batch_size", tcfg.batch_size))
        tcfg.log_interval      = int(sch.get("log_interval", tcfg.log_interval))
        tcfg.save_interval     = int(sch.get("save_interval", tcfg.save_interval))

        tcfg.grad_accum_steps = int(amp.get("grad_accum_steps", tcfg.grad_accum_steps))

        dprint(f"[cfg] train.yaml loaded → n_step={tcfg.n_steps}, updates={tcfg.num_updates}, epochs={tcfg.epochs_per_update}, "
               f"batch={tcfg.batch_size}, log_interval={tcfg.log_interval}, save_interval={tcfg.save_interval}, grad_accum_steps={tcfg.grad_accum_steps} | "
               f"gamma={tcfg.gamma}, clip={tcfg.ppo_clip}, entropy_coef={tcfg.entropy_coef}, "
               f"lr_actor={tcfg.lr_actor}, lr_critic={tcfg.lr_critic}")
    except Exception as e:
        dprint("[cfg] train.yaml not loaded, using defaults:", e)
    # -----------------------------------

    # 누적 CSV 파일명 (환경변수로 지정 가능)
    RUN_NAME = os.getenv("RUN_NAME", "3dpp_experiment1")

    dprint("[boot] entering main()")
    train(tcfg, ecfg, run_name=RUN_NAME)
    dprint("[boot] main() returned normally")
